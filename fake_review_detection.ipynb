{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e39cac44-05d2-40fe-883b-1bd3e3fdbdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas openpyxl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d621a3f4-61cd-45dc-a2f8-5996ced00b88",
   "metadata": {},
   "source": [
    "pip install --force-reinstall numpy==1.26.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "91562db6-53e8-4964-a580-262dcf3f8cd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from seaborn) (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ea6da000-d022-452f-b54d-75a26b5f5e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string, nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23105a6-a6a6-42f4-9dce-99ddbc3dbf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data AI.xlsx', engine='openpyxl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6308f9ac-adc3-4c93-a86f-80773ea0c90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4998 entries, 0 to 4997\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Unnamed: 0  0 non-null      float64\n",
      " 1   Unnamed: 1  0 non-null      float64\n",
      " 2   STT         4998 non-null   int64  \n",
      " 3   category    4998 non-null   object \n",
      " 4   rating      4998 non-null   int64  \n",
      " 5   label       4998 non-null   object \n",
      " 6   review      4997 non-null   object \n",
      "dtypes: float64(2), int64(2), object(3)\n",
      "memory usage: 273.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "68b4d494-4219-42a0-8d17-098389d61307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: underthesea in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (6.8.4)\n",
      "Requirement already satisfied: Click>=6.0 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from underthesea) (8.2.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.6 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from underthesea) (0.9.11)\n",
      "Requirement already satisfied: nltk in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from underthesea) (3.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from underthesea) (4.67.1)\n",
      "Requirement already satisfied: requests in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from underthesea) (2.32.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from underthesea) (1.5.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from underthesea) (1.6.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from underthesea) (6.0.2)\n",
      "Requirement already satisfied: underthesea-core==1.0.4 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from underthesea) (1.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from Click>=6.0->underthesea) (0.4.6)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from nltk->underthesea) (2024.11.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from requests->underthesea) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from requests->underthesea) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from requests->underthesea) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from requests->underthesea) (2025.4.26)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from scikit-learn->underthesea) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from scikit-learn->underthesea) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from scikit-learn->underthesea) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install underthesea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28573303-9ee1-4b0f-a902-f97255528ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yêu thật sự! Làm rất chắc chắn, bền và cực kỳ ...</td>\n",
       "      <td>[Yêu, thật sự, ! Làm, rất, chắc chắn, ,, bền, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rất đẹp, mình mê luôn, là một bản nâng cấp tuy...</td>\n",
       "      <td>[Rất, đẹp, ,, mình, mê, luôn, ,, là, một, bản,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cái gối này đã cứu sống lưng tôi. Tôi rất thíc...</td>\n",
       "      <td>[Cái, gối, này, đã, cứu, sống lưng, tôi, ., Tô...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thiếu hướng dẫn sử dụng, nhưng với mức giá này...</td>\n",
       "      <td>[Thiếu, hướng dẫn, sử dụng, ,, nhưng, với, mức...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bộ này rất đẹp. Chất lượng tốt. Gia đình tôi đ...</td>\n",
       "      <td>[Bộ, này, rất, đẹp, ., Chất lượng, tốt, ., Gia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  Yêu thật sự! Làm rất chắc chắn, bền và cực kỳ ...   \n",
       "1  Rất đẹp, mình mê luôn, là một bản nâng cấp tuy...   \n",
       "2  Cái gối này đã cứu sống lưng tôi. Tôi rất thíc...   \n",
       "3  Thiếu hướng dẫn sử dụng, nhưng với mức giá này...   \n",
       "4  Bộ này rất đẹp. Chất lượng tốt. Gia đình tôi đ...   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [Yêu, thật sự, ! Làm, rất, chắc chắn, ,, bền, ...  \n",
       "1  [Rất, đẹp, ,, mình, mê, luôn, ,, là, một, bản,...  \n",
       "2  [Cái, gối, này, đã, cứu, sống lưng, tôi, ., Tô...  \n",
       "3  [Thiếu, hướng dẫn, sử dụng, ,, nhưng, với, mức...  \n",
       "4  [Bộ, này, rất, đẹp, ., Chất lượng, tốt, ., Gia...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from underthesea import word_tokenize\n",
    "\n",
    "# Function to tokenize text using underthesea\n",
    "def tokenize_text(text):\n",
    "    # Check if the input is a string, if not return an empty string\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Tokenize the text using underthesea library\n",
    "    words = word_tokenize(text)\n",
    "    return words\n",
    "\n",
    "# Apply the 'tokenize_text' function to the entire 'review' column\n",
    "df['tokenized_text'] = df['review'].apply(tokenize_text)\n",
    "\n",
    "# Display the tokenized results\n",
    "df[['review', 'tokenized_text']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8076cd0-c2e9-40e0-b22c-c026327c069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas to display all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Save the tokenization result to a CSV file\n",
    "df[['review', 'tokenized_text']].to_csv('tokenized_text_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "51445511-1664-4f42-856f-6e57301a7e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6902aa89-5ea6-483f-aadb-e842065b8f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yêu thật sự! Làm rất chắc chắn, bền và cực kỳ ...</td>\n",
       "      <td>[Yêu, thật sự,  Làm, chắc chắn, , bền, thoải m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rất đẹp, mình mê luôn, là một bản nâng cấp tuy...</td>\n",
       "      <td>[đẹp, , mê, , một, bản, nâng cấp, tuyệt vời, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cái gối này đã cứu sống lưng tôi. Tôi rất thíc...</td>\n",
       "      <td>[Cái, gối, này, cứu, sống lưng, , thích, cảm g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thiếu hướng dẫn sử dụng, nhưng với mức giá này...</td>\n",
       "      <td>[Thiếu, hướng dẫn, sử dụng, , mức, giá, này, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bộ này rất đẹp. Chất lượng tốt. Gia đình tôi đ...</td>\n",
       "      <td>[Bộ, này, đẹp, , Chất lượng, tốt, , Gia đình, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  Yêu thật sự! Làm rất chắc chắn, bền và cực kỳ ...   \n",
       "1  Rất đẹp, mình mê luôn, là một bản nâng cấp tuy...   \n",
       "2  Cái gối này đã cứu sống lưng tôi. Tôi rất thíc...   \n",
       "3  Thiếu hướng dẫn sử dụng, nhưng với mức giá này...   \n",
       "4  Bộ này rất đẹp. Chất lượng tốt. Gia đình tôi đ...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  [Yêu, thật sự,  Làm, chắc chắn, , bền, thoải m...  \n",
       "1  [đẹp, , mê, , một, bản, nâng cấp, tuyệt vời, p...  \n",
       "2  [Cái, gối, này, cứu, sống lưng, , thích, cảm g...  \n",
       "3  [Thiếu, hướng dẫn, sử dụng, , mức, giá, này, m...  \n",
       "4  [Bộ, này, đẹp, , Chất lượng, tốt, , Gia đình, ...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vietnamese_stopwords = [\n",
    "\"tôi\", \"bạn\", \"chúng tôi\", \"chúng ta\", \"hắn\", \"nó\", \"họ\", \"mình\", \"ta\", \"các\", \"đây\", \"đó\", \"kia\",\n",
    "\"đang\", \"sẽ\", \"đã\", \"từng\", \"mới\", \"luôn\", \"thường\", \"nữa\", \"rồi\", \"lúc\", \"khi\", \"mỗi\", \"mọi\", \"vừa\",\n",
    "\"và\", \"nhưng\", \"hoặc\", \"vì\", \"nếu\", \"thì\", \"là\", \"mà\", \"nên\", \"bởi\", \"tuy\", \"dù\", \"với\", \"do\", \"để\", \"theo\",\n",
    "\"trong\", \"ngoài\", \"trên\", \"dưới\", \"từ\", \"đến\", \"qua\", \"về\", \"tại\", \"giữa\", \"quanh\", \"bởi\",\n",
    "\"chẳng\", \"rất\", \"quá\", \"hơi\", \"lắm\", \"ít\", \"nhiều\", \"của\", \"vẫn\", \"cực\", \"cực kỳ\",\n",
    "\"ai\", \"gì\", \"nào\", \"sao\", \"đâu\", \"à\", \"ư\", \"hả\", \"nhé\", \"nha\", \"vậy\", \"ừ\", \"à\", \"ồ\", \"ôi\"\n",
    "     ]\n",
    "\n",
    "# Function to remove stopwords and punctuation\n",
    "def remove_stopwords(words):\n",
    "    return [word for word in words if word.lower() not in vietnamese_stopwords]\n",
    "\n",
    "def remove_punctuation_from_words(words):\n",
    "    # Apply re.sub to each word in the list\n",
    "    return [re.sub(r'[.,!?]', '', word) for word in words if isinstance(word, str)]\n",
    "\n",
    "\n",
    "# Apply 'remove_stopwords' and 'remove_punctuation_from_words' to the entire 'tokenized_text' column\n",
    "df['cleaned_text'] = df['tokenized_text'].apply(remove_punctuation_from_words).apply(remove_stopwords)\n",
    "\n",
    "# Display the result after stopword and punctuation removal\n",
    "df[['review', 'cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ce0ca3-b801-4eba-b57b-52100607cf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save stopword results to CSV file\n",
    "df[['review', 'cleaned_text']].to_csv('cleaned_text_output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b5347611-cefa-4bc9-91a3-f5e5a4770b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\acer\\venv\\test_venv\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7d6efa-588f-4d25-973e-25a03db5f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "#stopwords have been removed and 'cleaned_text' contains tokenized sentences without stopwords\n",
    "tokenized_texts = df['cleaned_text'].tolist()  # List of sentences, each sentence is a list of words\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"word2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3201a703-6172-42c8-ade4-739fb4554631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4998, 100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to convert each sentence to a vector by averaging the word vectors in the sentence\n",
    "def sentence_to_vec(sentence, model):\n",
    "    # Check if sentence is a list of words (tokenized text)\n",
    "    if not isinstance(sentence, list):\n",
    "        sentence = sentence.split()  # Split into words if it's a string\n",
    "\n",
    "    # Get the words in the sentence that exist in the model's vocabulary\n",
    "    words = [word for word in sentence if word in model.wv]\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(model.vector_size)  # Return a zero vector if no words are in the model\n",
    "    return np.mean([model.wv[word] for word in words], axis=0)\n",
    "\n",
    "# Convert the entire dataset (e.g., 5,000 reviews) into vectors\n",
    "vectors = np.array([sentence_to_vec(sentence, model) for sentence in df['cleaned_text']])\n",
    "\n",
    "# Check the shape of the vectors array\n",
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1ecab-90c8-4776-966c-ec44f6a9ab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the vectors (each row is a vector from a review)\n",
    "vectors_df = pd.DataFrame(vectors)\n",
    "\n",
    "# Save the vectors to a CSV file\n",
    "vectors_df.to_csv('review_vectors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cab337-d9c1-419a-9e64-ad25e3447491",
   "metadata": {},
   "source": [
    "Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dc97e4-3a3a-4cdb-818f-afa0bc87eba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.679\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Input data: Sentence vectors\n",
    "X = vectors\n",
    "# Classification labels (fake/real)\n",
    "y = df['label']\n",
    "\n",
    "# Split data into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the RandomForest model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1043012f-5576-4b56-a978-7e7ca8d8af25",
   "metadata": {},
   "source": [
    "Random Forest test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a2beac-40d9-41cf-88a3-5a0f99727146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dự đoán nhãn: ['OR']\n"
     ]
    }
   ],
   "source": [
    "# Function to convert a sentence into a vector by averaging the word vectors in the sentence\n",
    "def sentence_to_vec(sentence, model):\n",
    "    # Get words in the sentence and retrieve their vectors\n",
    "    words = [word for word in sentence if word in model.wv]  # Only use words available in the model\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(model.vector_size)  # Return 0 vector if no words are in the model\n",
    "    return np.mean([model.wv[word] for word in words], axis=0)\n",
    "\n",
    "# Example of a new review sentence\n",
    "new_review = \"Sản phẩm này rất tuyệt vời, tôi rất thích!\"\n",
    "\n",
    "# Preprocess and convert the new sentence into a vector (same as done with training data)\n",
    "new_review_vector = sentence_to_vec(new_review.split(), model)  # Chuyển câu thành vector bằng Word2Vec\n",
    "\n",
    "# Predict the label for the new review sentence using the trained model\n",
    "predicted_label = clf.predict([new_review_vector])\n",
    "\n",
    "# Print the predicted label\n",
    "print(\"Dự đoán nhãn:\", predicted_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb43d37e-3ba3-4cf5-8229-9d4479581109",
   "metadata": {},
   "source": [
    "Random Forest optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a2bb6f-2440-4401-badb-00e940deeab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "Best parameters: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100, 'random_state': 42}\n",
      "Optimized Random Forest Accuracy: 0.672\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],               # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30],              # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],               # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],                 # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],     # Number of features to consider when looking for the best split\n",
    "    'random_state': [42]                          # Ensures reproducibility\n",
    "}\n",
    "\n",
    "# Initialize the RandomForest model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model with the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters \n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Predict and evaluate the optimized model\n",
    "y_pred_rf_optimized = grid_search.predict(X_test)\n",
    "print(\"Optimized Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf_optimized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1afcbc-b34d-4438-bd79-36e1179b2666",
   "metadata": {},
   "source": [
    "SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60de5478-cea1-4623-8c2b-329b24220c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy của mô hình SVM: 0.639\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the SVM model\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Train the SVM model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "# Display the accuracy of the SVM model\n",
    "print(\"Accuracy của mô hình SVM:\", accuracy_svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fb53a4-639f-4685-b2f9-b52bdd19d396",
   "metadata": {},
   "source": [
    "SVM test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d65960-9bcd-40b3-98a8-4fc117a09603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dự đoán nhãn: ['CG']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to convert a sentence into a vector by averaging the word vectors in the sentence\n",
    "def sentence_to_vec(sentence, model):\n",
    "    # Retrieve words in the sentence and their corresponding vectors\n",
    "    words = [word for word in sentence if word in model.wv]  # Ensure only words present in the model are used\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(model.vector_size)  # Return 0 vector if no words are found in the model\n",
    "    return np.mean([model.wv[word] for word in words], axis=0)\n",
    "\n",
    "# Example new review sentence\n",
    "new_review = \"Sản phẩm này rất tuyệt vời, tôi rất thích!\"\n",
    "\n",
    "# Preprocess and convert the new sentence into a vector (same as with the training data)\n",
    "new_review_vector = sentence_to_vec(new_review.split(), model)  # Chuyển câu thành vector bằng Word2Vec\n",
    "\n",
    "# Predict the label for the new review \n",
    "predicted_label = svm_model.predict([new_review_vector])\n",
    "\n",
    "# Print the predicted label\n",
    "print(\"Dự đoán nhãn:\", predicted_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e8e587-b65d-4b71-a7f7-82f1f5e418a3",
   "metadata": {},
   "source": [
    "SVM optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de1f497-5d63-4cc4-bdbc-cff9212033ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best parameters: {'C': 100, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Optimized SVM Accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Initialize the SVM model\n",
    "svm_model = SVC(random_state=42)\n",
    "\n",
    "# Perform hyperparameter tuning using GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Train the model with GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Predict using the optimized model\n",
    "y_pred_svm_optimized = grid_search.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Optimized SVM Accuracy:\", accuracy_score(y_test, y_pred_svm_optimized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6c71de-7d2c-402d-b572-d8eba3317eb1",
   "metadata": {},
   "source": [
    "Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36e1533-2905-414c-8316-52c5ac1b7089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.647\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f3676b-0bae-4924-8519-f288f2e9d9cb",
   "metadata": {},
   "source": [
    "Logistic Regression test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225ed68d-1c02-4ed6-98e7-9450cd6f2cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dự đoán nhãn: ['CG']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to convert a sentence into a vector by averaging the word vectors in the sentence\n",
    "def sentence_to_vec(sentence, model):\n",
    "    # Extract words from the sentence and get their vectors\n",
    "    words = [word for word in sentence if word in model.wv]  # Ensure only words that exist in the model are used\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(model.vector_size)  # Return 0 vector if no words are found in the model\n",
    "    return np.mean([model.wv[word] for word in words], axis=0)\n",
    "\n",
    "# Example of a new review sentence\n",
    "new_review = \"Sản phẩm này rất tuyệt vời, tôi rất thích!\"\n",
    "\n",
    "# Preprocess and convert the new sentence into a vector \n",
    "new_review_vector = sentence_to_vec(new_review.split(), model)  # Convert sentence into vector using Word2Vec\n",
    "\n",
    "# Predict the label for the new review Logistic Regression model\n",
    "predicted_label = lr_model.predict([new_review_vector])\n",
    "\n",
    "# Print the predicted label\n",
    "print(\"Dự đoán nhãn:\", predicted_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa513f87-85b0-4316-9ec5-868e1c331897",
   "metadata": {},
   "source": [
    "Logistic Regression optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71581d7-a494-4354-980a-bc010d6a95b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best parameters: {'C': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Logistic Regression Accuracy: 0.722\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for optimization\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength (inverse of regularization)\n",
    "    'penalty': ['l1', 'l2'],        # Type of regularization: L1 or L2\n",
    "    'solver': ['liblinear', 'saga'] # Optimization algorithms that support L1 or L2\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=lr_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Train the model using GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Predict and evaluate the optimized model\n",
    "y_pred_lr = grid_search.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "665f8d17-7113-48bf-b1db-4f2940608008",
   "metadata": {},
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "raw",
   "id": "667bdf52-89d7-4ac0-9850-b6fb9d157000",
   "metadata": {},
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6648c60b-5af8-4f63-bedd-17ffb2fa0160",
   "metadata": {},
   "source": [
    "pip install transformers torch underthesea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91f2cfc-221b-412a-8d1a-898a94b925e8",
   "metadata": {},
   "source": [
    "phoBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d556da2-e80e-476d-a5fb-486354ce77c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhoBERT + RandomForest Accuracy: 0.722\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load PhoBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "def get_phobert_embedding(text):\n",
    "    # Tokenize and encode\n",
    "    input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n",
    "    with torch.no_grad():\n",
    "        features = phobert(input_ids)\n",
    "        # Use the [CLS] token representation as the sentence embedding\n",
    "        embedding = features.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "    return embedding\n",
    "\n",
    "# Apply PhoBERT embedding to all reviews\n",
    "df['phobert_vec'] = df['review'].fillna('').apply(get_phobert_embedding)\n",
    "\n",
    "# Convert list of embeddings to numpy array\n",
    "X_phobert = np.stack(df['phobert_vec'].values)\n",
    "y = df['label']\n",
    "\n",
    "# Train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_phobert, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a classifier (e.g., RandomForest)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"PhoBERT + RandomForest Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d631d6ab-c2b5-4c0e-ae77-04d731dca206",
   "metadata": {},
   "source": [
    "phoBERT test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf3952-ed7f-4958-9db8-ff7704b54d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dự đoán nhãn cho review mới: ['OR']\n"
     ]
    }
   ],
   "source": [
    "# Example of a new review\n",
    "new_review = \"Sản phẩm này rất tuyệt vời, tôi rất thích!\"\n",
    "\n",
    "# Get PhoBERT embedding for the new review\n",
    "new_vec = get_phobert_embedding(new_review)\n",
    "\n",
    "# Predict label using the trained model\n",
    "pred_label = clf.predict([new_vec])\n",
    "print(\"Dự đoán nhãn cho review mới:\", pred_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd98b85d-2a11-4d09-a5a9-8eda9ea69605",
   "metadata": {},
   "source": [
    "phoBERT optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f966e05e-36b1-45f5-a300-3c48f7f5a748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "Best parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200, 'random_state': 42}\n",
      "Optimized Random Forest Accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with RandomForestClassifier\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "# Train the model using GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the optimized model on the test set\n",
    "y_pred_optimized = grid_search.predict(X_test)\n",
    "print(\"Optimized Random Forest Accuracy:\", accuracy_score(y_test, y_pred_optimized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cb8e09-4d4c-4109-8046-aa33c6d4da9d",
   "metadata": {},
   "source": [
    "Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2551014a-a1ba-4a82-af14-9e0c5f689f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.608\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the Decision Tree model\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cab272-7570-42d9-83f3-1dc3c50c3d36",
   "metadata": {},
   "source": [
    "Decision tree test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfab6da2-a4b3-4dc2-95e0-f593c23ca176",
   "metadata": {},
   "source": [
    "Decision tree optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b9268f40-1fa9-4ce2-973e-9a1506f2e292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "Best parameters for Decision Tree: {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'random_state': 42}\n",
      "Optimized Decision Tree Accuracy: 0.606\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Định nghĩa không gian tham số để tối ưu hóa\n",
    "param_grid = {\n",
    "    'max_depth': [None, 5, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Khởi tạo GridSearchCV cho Decision Tree\n",
    "grid_search_dt = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search_dt.fit(X_train, y_train)\n",
    "\n",
    "# In ra tham số tối ưu\n",
    "print(\"Best parameters for Decision Tree:\", grid_search_dt.best_params_)\n",
    "\n",
    "# Đánh giá trên tập test\n",
    "y_pred_dt_optimized = grid_search_dt.predict(X_test)\n",
    "print(\"Optimized Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt_optimized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf3514-bbe9-4a75-9770-27068f24cafd",
   "metadata": {},
   "source": [
    "XGBoots model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec239996-4c56-452e-9482-55bafc0987b4",
   "metadata": {},
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec009b3-61ea-4e77-b1df-587388d278be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.742\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode class labels as integers\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "# Initialize and train the XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e204bb-cd2c-4771-99a8-ee505ce02ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dự đoán nhãn với XGBoost: [1]\n"
     ]
    }
   ],
   "source": [
    "# Test with new review\n",
    "new_review = \"Sản phẩm này rất tuyệt vời, tôi rất thích!\"\n",
    "\n",
    "new_vec = get_phobert_embedding(new_review)  # This function returns a 768-dimensional vector\n",
    "\n",
    "# Predict the label using XGBoost\n",
    "predicted_label_xgb = xgb_model.predict([new_vec])\n",
    "print(\"Dự đoán nhãn với XGBoost:\", predicted_label_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b72c92-04da-4b89-9216-a100f5907f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best parameters for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 200, 'random_state': 42, 'subsample': 0.8}\n",
      "Optimized XGBoost Accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for XGBoost using GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV for XGBoost\n",
    "grid_search_xgb = GridSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='logloss'), param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best parameters for XGBoost:\", grid_search_xgb.best_params_)\n",
    "\n",
    "# Predict on the test set using the optimized model\n",
    "y_pred_xgb_optimized = grid_search_xgb.predict(X_test)\n",
    "print(\"Optimized XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb_optimized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c4d18a-7b3c-4141-9110-475f34b151cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python311",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
